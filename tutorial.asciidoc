:language: r
:source-highlighter: rouge
:numbered:
:icons: font
:toc: left
:important-caption: :heavy_exclamation_mark:
:experimental:

= Analysis of genotyping-by-sequencing data

++++
<link rel="stylesheet"  href="http://cdnjs.cloudflare.com/ajax/libs/font-awesome/3.1.0/css/font-awesome.min.css">
++++

////
[#img-overview]
.Workflow of the GBS analysis pipeline. 
image::https://bitbucket.org/tritexassembly/tritexassembly.bitbucket.io/raw/9375957ff5f1763b1ce11d090919a76de9d7bf7a/tritex_overview.jpg[]
////

image::2024_GBStutorial_flowchart.jpeg[scaledwidth="75%", align=center]

== Open a UNIX shell

. Connect with https://en.wikipedia.org/wiki/PuTTY[PuTTY] to the server slurm-14.ipk-gatersleben.de.

. Obtain a Kerberos ticket to access our filer resources.
+
[source,sh]
----
kinit 
klist <1>
----
<1> A ticket is valid for seven days.

. You will see the prompt of the UNIX command-line interpreter, the https://en.wikipedia.org/wiki/Unix_shell[shell].
It is called "shell" because it executes commands that do not belong to the operating system, the "kernel".

. Check out your home directory.
+
[source,sh]
----
pwd
ls
----

== Set up your analysis directory  ==

. Let's have a look at the GBS data.
+
[source,sh]
----
cd /filer/projekte/gbs_course
cd data
ls
ls -lhS gbs_reads | less <1>
man ls
cd gbs_reads
ls | wc -l
zcat HOR_337.fastq.gz | less -S // <2>
cd ..
du -ch gbs_reads
----
<1> To close `less`, press `q`.
<2> The reads are in https://en.wikipedia.org/wiki/FASTQ_format[FASTQ format].

. Have a look at the passport data.
+
[source,sh]
----
less core200_passport_data.tsv
column -t core200_passport_data.tsv | less -S
----

. Use WinSCP to copy the XLSX file with the passport data to your local computer.

. Create your working directory and change into it.
+
[source,sh]
----
cd /filer/projekte/gbs_course
mkdir -p mascher/gbs // <1>
cd mascher/gbs 
----
<1> Change `mascher` to your name.

. Create symbolic links to the GBS reads and the phenotype data.
+
[source,sh]
----
pwd // <1>

mkdir gbs_reads
datadir="/filer/projekte/gbs_course/data/"
ln -s $datadir/gbs_reads/*fastq.gz gbs_reads
ls -l gbs_reads | less -S
ls gbs_reads | wc -l 
----
<1> Check that you are in your working directory (`/filer/projekte/gbs_course/USER/gbs`).

. Count the reads in one FASTQ file.
+
[source,r]
----
zcat gbs_reads/HOR_337.fastq.gz | wc -l > HOR_337.len.txt
cat HOR_337.len.txt
----

. We use a https://en.wikipedia.org/wiki/For_loop[for-loop] to get the read counts of all samples in one go.
+
[source,r]
----
for i in gbs_reads/*fastq.gz; do
 name=`basename $i | cut -d . -f 1`
 count=`zcat $i | wc -l`
 echo $name $count
done > fastq_line_counts.txt
----

. Add column with read count (as opposed to the line count).
+
[source,r]
----
cat fastq_line_counts.txt | awk '{print $1,$2/4}' | sort -nk 2 > fastq_read_counts.txt // <1>
----
<1> We use https://www.tutorialspoint.com/awk/index.htm[AWK] to divide the second column by 4. 

== Read mapping ==

. We will use the tools https://github.com/lh3/bwa[BWA] and http://www.htslib.org/doc/samtools.html[samtools] for read mapping and processing of alignment records, respectively.
+
[source,sh]
----
module load bwa samtools
module list
----

. We will first run a toy example with 10,000 reads of the HOR 337 sample.
+
[source,sh]
----
zcat gbs_reads/HOR_337.fastq.gz | head -n 40000 > HOR_337_10k.fastq

ref='/filer/projekte/gbs_course/data/reference/Hordeum_vulgare_MorexV3_assembly.fasta' // <1>
bwa mem $ref HOR_337_10k.fastq > HOR_337_10k.sam <2>
samtools sort HOR_337_10k.sam > HOR_337_10k.bam <2>
samtools view HOR_337_10k.bam | less -S
----
<1> We use the https://academic.oup.com/plcell/article/33/6/1888/6169005[MorexV3] reference genome sequence.
<2> Alignments are stored in the https://en.wikipedia.org/wiki/SAM_(file_format)[Sequence Alignment Map format] (SAM) or its binary equivalent BAM.

. Combine mapping and sorting into one command.
+
[source,sh]
----
bwa mem $ref HOR_337_10k.fastq | samtools sort > HOR_337_10k.bam
----

. Open a new PuTTY session. Start https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/[tmux] to keep your commands running in the background. 
+
[source,sh]
----
k5reauth tmux <1> 
----
<1> k5reauth is used to prolong the Kerberos ticket for as long as the tmux session is running.

. Try out tmux functionality.
+
[source,sh]
----
tmux ls
tmux rename mapping
tmux ls
tmux detach 
tmux ls
tmux attach -t mapping
----

. Map all samples.
+
[source,sh]
----
ref='/filer/projekte/gbs_course/data/reference/Hordeum_vulgare_MorexV3_assembly.fasta'
for i in gbs_reads/*fastq.gz; do
 name=`echo $i | cut -d . -f 1` // <1>
 bwa mem -t 12 $ref $i | samtools sort > $name.bam
done 2> bwa.err <2>
----
<1> Strip the extension: HOR_337.fastq.gz become HOR_337.
<2> To detach the tmux session, press kbd:[Ctrl-B] followed by kbd:[D]. 

+
IMPORTANT: If you forget to start bwa inside a tmux session, there is no way to prevent your job from aborting when you shutdown your laptop. Also without k5reauth programs cannot access filer resource after a maximum of ten hours.

. Open a new terminal. Look at your jobs in the table of processes (`top`).
+
[source,sh]
----
top -u mascher <1>
----
<1> Replace mascher with your username.

. Once the mapping is finished, count the number of BAM files.
+
[source,sh]
----
ls gbs_reads/*bam | wc -l
----

. If you have fewer than 200 files and the mapping didn't work for some reason, use the pre-computed files.
+
[source,sh]
----
ln -fst gbs_reads /filer-dg/agruppen/dg6/mascher/gbs_course2024_231222/data/bam_files/*bam
----

. Count the number of mapped reads with a minimum quality score of 20 for the HOR 337 sample and compare the result to the input read count of that sample.
+
[source,sh]
----
samtools view -cq 20 -cq20 gbs_reads/HOR_337.bam
grep HOR_337 fastq_read_counts.txt
----

. Get mapped read counts for all samples.
+
[source,sh]
----
for i in gbs_reads/*bam; do
 name=`basename $i | cut -d . -f 1`
 count=`samtools view -cq 20 $i`
 echo $name $count
done > mapped_reads.txt
----

. Combine the raw read counts and the mapping rates into one table.
+
[source,sh]
----
LC_ALL=C sort fastq_read_counts.txt > tmp1 // <1>
LC_ALL=C sort mapped_reads.txt > tmp2 // <1>

join tmp1 tmp2 | awk '{print $0,$3/$2*100}' | sort -nk 4 | column -t > mapping_stats.txt // <2>

rm -f tmp1 tmp2

column -t mapping_stats.txt  | less -S
----
<1> To combine two lists with https://linux.die.net/man/1/join[join], both lists need to be sorted on the common ID column.
<2> https://linux.die.net/man/1/column[column] is used to align columns.

== Variant calling and filtration ==

. Open a new tmux session and load https://samtools.github.io/bcftools/howtos/index.html[bcftools].
+
[source,sh]
----
tmux // <1>
tmux rename variant_call
module load bcftools
----
<1> The variant calling will run for some time, so run it inside `tmux`.

. Get a list of all BAM files.
+
[source,sh]
----
ls gbs_reads/*bam | sort > bam_list.txt
----

. Run the variant calling.
+
[source,sh]
----
ref='/filer/projekte/gbs_course/data/reference/Hordeum_vulgare_MorexV3_assembly.fasta'
bamlist='/filer-dg/agruppen/dg6/mascher/gbs_course2024_231222/try_231222/bam_list.txt' // <1>
vcf='bcftools_SNP_calling.vcf' // <2>

bcftools mpileup --bam-list $bamlist --skip-indels --fasta-ref $ref --min-MQ 20 --annotate AD | bcftools call --consensus-caller --variants-only --output $vcf // <3>
----
<1> List of pre-computed BAM files.
<2> Output file in https://en.wikipedia.org/wiki/Variant_Call_Format[variant call format] (VCF). https://samtools.github.io/hts-specs/VCFv4.2.pdf[Here] are the specifications of the VCF format.
<3> We ignore insertions and deletions (`--skip-indels`), consider only SNPs with a quality score no smaller than 20 (`--min-MQ 20`) and add allelic depth information (`--annotate AD`) for all genotype calls.

. Filter the variant calls.
+
[source, sh]
----
filter='/filer/projekte/gbs_course/scripts/filter_vcf.zsh'
vcf='/filer-dg/agruppen/dg6/mascher/gbs_course2024_231222/try_231222/bcftools_SNP_calling.vcf' <1>
fvcf='bcftools_SNP_calling_filtered.vcf'

$filter --vcf $vcf --dphom 2 --dphet 4 --minmaf 0.2 --minpresent 0.9 --minhomp 0.9 > $fvcf // <2>
----
<1> Path to pre-computed VCF file.
<2> We keep homozygous genotype calls if they have at least two supporting reads; heterozygous calls are accepted if they are supported by no fewer than four reads. SNPs with a minor allele frequency below 20 % or less than 90 % present calls or less than 90 % homozygous calls are discarded.

. Review the VCF file.
+
[source,sh]
----
grep -v '^##' bcftools_SNP_calling_filtered.vcf | column -t | less -S
----

== Principal component analysis

. Open R. 
+
[source,sh]
----
module load R/3.5.1
R 
----

. R is a widely used programming language in data science. There are many tutorials, e.g. https://www.statmethods.net/r-tutorial/index.html[this one].

. Load the required libraries.
+
[source,r]
----
.libPaths(c("/filer-dg/agruppen/seq_shared/mascher/Rlib/3.5.1", "/opt/Bio/R_LIBS/3.5.1")) // <1>

library(data.table) // <2>
library(SeqArray) // <3>
library(SNPRelate) // <3>
library(countrycode) // <4>

----
<1> Set the paths where the R libraries are located.
<2> https://cran.r-project.org/web/packages/data.table/index.html[data.table] extends R core functionality when handling large tables.
<3> https://academic.oup.com/bioinformatics/article/33/15/2251/3072873[seqArray] and https://academic.oup.com/bioinformatics/article/28/24/3326/245844[SNPRelate] are two R packages to store and analyze SNP matrices.
<4> The countrycode package will be used to make country abbreviations to geographic regions.

. Convert the VCF file to the binary GDS (Genomic Data Structure) format used by seqArray and SNPRelate.
+
[source,r]
----
seqVCF2GDS(vcf.fn='/filer-dg/agruppen/dg6/mascher/gbs_course2024_231222/try_231222/bcftools_SNP_calling_filtered.vcf', out.fn='bcftools_SNP_calling_filtered.gds') // <1>
----
<1> This creates the GDS file in the current working directory.

. Open the GDS file and get summary statistics.
+
[source,r]
----
seqOpen('bcftools_SNP_calling_filtered.gds') -> gds
seqSummary(gds)
----

. Run a principal components analysis (PCA) on the data and extract the eigenvectors.
+
[source,r]
----
snpgdsPCA(gds, autosome.only=F) -> pca // <1>

data.table(pca$sample.id, pca$eigenvect[, 1:2]) -> ev
setnames(ev, c("accession", "PC1", "PC2")) // <2>
ev[, accession := sub(".bam$", "", sub(".*/", "", accession))] // <3>
----
<1> `autosome.only=F` is needed because chromsomes are named chr1H, chr2H ... instead of 1, 2, ...
<2> Set proper column names.
<3> Change the sample names inherited from the VCF file (BAM file names).

. Read the passport data for the core200 panel and merge them with the PCA results.
+
[source,r]
----
data.table(read.xlsx("/filer-dg/agruppen/dg6/mascher/gbs_course2024_231222/data/core200_passport_data.xlsx")) -> pp
pp[ev, on="accession"] -> ev
----

. Plot the first two principal components (PCs) with samples coloured by row type.
+
[source,r]
----
ev[, col := "gray"] // <1>
ev[row_type == "6-rowed", col := "black"] // <2>
ev[row_type == "2-rowed", col := "red"]

pdf("PCA1.pdf") // <3>
ev[, plot(PC1, PC2, col=col, pch=19, xlab="PC1", ylab="PC2")] // <4>
dev.off() // <3>
----
<1> Add a color column. The default color is gray.
<2> If the the row type is six-rowed, set the color to black.
<3> Open a PDF file as the plot device and close it after the plot function has been called.
<4> `pch` specifies the http://www.sthda.com/english/wiki/r-plot-pch-symbols-the-different-point-shapes-available-in-r[plotting symbol]. 19 means solid circles.

. Repeat with samples colored by annual growth habit.
+
[source,r]
----
ev[, col2 := "gray"]
ev[annual_growth_habit == "spring", col2 := "black"]
ev[annual_growth_habit  == "winter", col2 := "red"]

pdf("PCA2.pdf")
ev[, plot(PC1, PC2, col=col2, pch=19, xlab="PC1", ylab="PC2")]
dev.off()

----

. Map the countries to continents.
+
[source,r]
----
ev[, countrycode(country_of_origin, "iso3c", "continent")] // <1>
ev[country_of_origin %in% c("DDR", "GER"), country_of_origin := "DEU"] 
ev[country_of_origin == "SUN", country_of_origin := "RUS"]
ev[country_of_origin == "CSK", country_of_origin := "CZE"]

ev[, continent := countrycode(country_of_origin, "iso3c", "continent")] 
ev[is.na(continent)] // <2>
----
<1> Some country code are invalid. Correct these.
<2> Check for missing data.

. Color according to country and plot the PCA again.
+
[source,r]
----
ev[, col3 := "gray"]
ev[continent == "Europe", col3 := "black"]
ev[continent == "Asia", col3 := "red"]
ev[continent == "Africa", col3 := "blue"]
ev[continent == "Americas", col3 := "green"]

pdf("PCA3.pdf")
ev[, plot(PC1, PC2, col=col3, pch=19, xlab="PC1", ylab="PC2")]
dev.off()
----

. Add a title and legend. Change the orientation of the y-axis labels.
+
[source,r]
----
pdf("PCA4.pdf")
ev[, plot(PC1, PC2, col=col3, pch=19, xlab="PC1", ylab="PC2", las=1, main="PCA colored by geography")] // <1>
legend("bottomright", col=c("black", "red", "blue", "green"), pch=19, legend=c("Europe", "Asia", "Africa", "Americas"))
dev.off()
----
<1> `las=1` means "always horizontal".

//. Load the BWA index into shared memory. This step can be skipped, but pre-loadeding the index makes mapping a lot faster.
//+
//source,sh]
//----
//ref='/filer/projekte/gbs_course/data/reference/Hordeum_vulgare_MorexV3_assembly.fasta' // <1>
//bwa shm $ref
//bwa shm -l
//----


